<h1 align = "center">
  Multimodal-Robustness-Benchmark
</h1>

<p align="center">
    <a href="https://arxiv.org/abs/2406.04264">
            <img alt="Build" src="http://img.shields.io/badge/cs.CV-arXiv%3A2406.04264-B31B1B.svg">
    </a>
    <a href="https://huggingface.co/datasets/BAAI/Multimodal-Robustness-Benchmark">
        <img alt="Build" src="https://img.shields.io/badge/ü§ó Dataset-MMR Benchmark-yellow">
    </a>
</p>

This repo contains the official evaluation code and dataset for the paper‚ÄúSeeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions‚Äù.

## üöÄ News and Updates

* 2024.06.15 üî• **ArXiv paper is released!**.

## Contents
- [MMR-benchmark](#MMR-benchmark)
- [Evaluation](#Evaluation)
- [Leaderboard](#Leaderboard)
- [MMR-data](#MMR-data)
- [Training](#Training)
- [Citation](#Citation)
- [License](#license)
- [Acknowledgement](#acknowledgement)

## MMR-benchmark
## Evaluation
## Leaderboard
## MMR-data
## Training
## Citation
If you find this repository helpful, please cite the paper below.

```bibtex
@article{he2024bunny,
      title={Efficient Multimodal Learning from Data-centric Perspective}, 
      author={He, Muyang and Liu, Yexin and Wu, Boya and Yuan, Jianhao and Wang, Yueze and Huang, Tiejun and Zhao, Bo},
      journal={arXiv preprint arXiv:2402.11530},
      year={2024}
}
```

## License
This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses.
The content of this project itself is licensed under the [Apache license 2.0](./LICENSE).

## Acknowledgement

We build our project based on [LLaVA](https://github.com/haotian-liu/LLaVA): Large Language and Vision Assistant.
