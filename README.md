<p align="center">
  # Multimodal-Robustness-Benchmark
</p>

<p align="center">
  <img src="./icon.png" alt="Logo" width="350">
</p>

ğŸ“– [Paper](https://arxiv.org/abs/2402.11530) | ğŸ¤– [Dataset](https://www.modelscope.cn/datasets/BoyaWu10/Bunny-v1.0-data) | ğŸ¤— [Model](https://huggingface.co/BAAI/Bunny-v1_0-3B) 

This repo contains the official evaluation code and dataset for the paperâ€œSeeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questionsâ€.

## ğŸš€ News and Updates

* 2024.06.15 ğŸ”¥ **ArXiv paper is released!**.

## Contents
- [MMR-benchmark](#MMR-benchmark)
- [Evaluation](#Evaluation)
- [Leaderboard](#Leaderboard)
- [MMR-data generation](#MMR-data generation)
- [Training](#Training)
- [Citation](#Citation)
- [Examples](#examples)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)
- [License](#license)

## MMR-benchmark
## Evaluation
## Leaderboard
## MMR-data generation
## Training
## Citation
If you find this repository helpful, please cite the paper below.

```bibtex
@article{he2024bunny,
      title={Efficient Multimodal Learning from Data-centric Perspective}, 
      author={He, Muyang and Liu, Yexin and Wu, Boya and Yuan, Jianhao and Wang, Yueze and Huang, Tiejun and Zhao, Bo},
      journal={arXiv preprint arXiv:2402.11530},
      year={2024}
}
```

## License
This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses.
The content of this project itself is licensed under the [Apache license 2.0](./LICENSE).

## Acknowledgement

We build our project based on [LLaVA](https://github.com/haotian-liu/LLaVA): Large Language and Vision Assistant.
